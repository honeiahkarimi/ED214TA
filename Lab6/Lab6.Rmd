---
title: 'Lab 6'
author: "ED 214B"
date: "Winter 2024"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

setwd("~/Desktop/ED214_TA_Materials/ED214B/Lab6")
```

## Import Libraries

```{r}
library(readr) # importing csv
library(interactions) # interact plot
library(psych) # describe
library(emmeans) # emmeans
library(lm.beta) # lm beta
library(DescTools) # Post Hoc Test
library(tidyverse) #ggplot
```

## Lab 5 Review

### Dummy Coding Regression Results

```{r}
data1 <- read_csv("data1.csv")
data1 <- data1 %>% 
  mutate(Democrat=ifelse(Party==1,1,0),
         Republican=ifelse(Party==2,1,0))
dum <- lm(Likelihood~Democrat+Republican,data=data1)
summary(dum)
```

### Interpreting Regression Coefficients

In Lab 5, we learned that when we use dummy codes in a regression model, each beta coefficient is the mean difference between the group corresponding to that category and the reference group. We cannot compare across groups; we can only make comparisons between each category and the reference.

**How do we interpret this in APA format?**

A regression model was used to determine if there was a statistically significant relationship between **political party membership** and **the likelihood of voting in the next election**.  Political party membership was significant, such that **Democrats** were less likely to vote than **Independents** ($\beta = -13.357$, $p$ < .001), as were **Republicans** ($\beta = -7.78$, $p$ < .001). Independents were the most likely to vote, with a voter likelihood of 51.981. Political party membership explained 13.78% of the variation in voter likelihood $[F(2,497)=40.88, p < .001]$.  

**Note**: We cannot make comparisons between **Republicans** and **Democrats** because dummy coded regression coefficients are the **expected mean difference between that group and the reference category**. If we changed the reference category, these coefficients would change.  

### Interpreting post hoc tests from ANOVA models

Note that we use post hoc tests in an ANOVA model to compare each set of groups.

```{r}
data1$Party <- factor(data1$Party,
                      levels=c(1,2,3),
                      labels=c("Democrat","Republican","Independent"))
model <- aov(Likelihood~Party,data=data1)
PostHocTest(model,method="hsd")
```

```{r}
emmeans(model, ~Party)
```

**How do we interpret this in APA format?**

A one-way ANOVA was conducted to compare the effect of political party membership (Independent/Republican/Democrat) on voter likelihood.  Voter likelihood was calculated using responses to a survey about voter likelihood (M = 44.44, SD = 14.19, min = 4.07, max = 92.88; see Figure 1 below).  There was a significant effect of political party membership on voter likelihood $[F(2,497)=40.88, p < .01]$. Post hoc comparisons using a Tukey-HSD correction revealed significant differences between all three political parties at the .01 alpha level.  Independents were the most likely to vote (M = 52.0) followed by Republicans (M = 44.2) and Democrats (M = 38.6; see Figure 2 below).

**Note**: In this case, all of the means are statistically significantly different from each other, so we say *"...revealed significant differences between all three political parties."* If there are some that are not statistically significant, you should only report the ones that are. You could conclude by saying something along the lines of *"all other groups were not statistically significantly different from each other"*.

## Interactions and moderation

In the past, we've assumed that the effect of a variable is the same across all levels of the other variable.  This is the called "Assumption of Parallel Lines", which means that we assume that the slope coefficients are the same for each level of another variable.

```{r}
data2<- read_csv("data2.csv")
View(data2)
```

**Research Question**: Is there a relationship between a student's self esteem, gender, and their performance in math class?

**Testable hypothesis 1**: Do self esteem and gender predict math test score?

**Testable hypothesis 2**: Does gender moderate the relationship between self esteem and math test scores?

**Let's start by running the model without an interaction**

```{r}
mod1 <- lm(Math_Score~Self_Esteem+Gender,data=data2)
summary(mod1)
lm.beta(mod1)
```

*Note: For the Gender variable, male = 0 and female = 1*

**How would we interpret these unstandardized coefficients?**

- Controlling for gender, a one-unit increase in Self Esteem corresponds to a 1.971 unit increase in Math Scores. This slope is the same for both genders.
- Controlling for Self Esteem, women score 6.891 points lower on the math test, on average.  **This difference is the same across all levels of Self Esteem.**

**How would we interpret these standardized coefficients?**

- Controlling for gender, a one **standard deviation** increase in Self Esteem corresponds to a .746 **standard deviation** increase in Math scores. This slope is the same for both genders.
- *Interpretation of the categorical predictor's standardized coefficient is meaningless because standard deviation for categorical variables is meaningless*

**Proof**: *This difference is the same across all levels of Self Esteem.*
$$\hat{Math}=27.231-6.891*Gender_1+1.971*Self-Esteem_i$$

Difference between male and female math scores when self-esteem score is 10:

- Male: $\hat{Math}=27.231-6.891*0+1.971*10$ = 46.941
- Female: $\hat{Math}=27.231-6.891*1+1.971*10$ = 40.05
- 46.941 - 40.05 = 6.891

Difference between male and female math scores when self-esteem score is 20:

- Male: $\hat{Math}=27.231-6.891*0+1.971*20$ = 66.651
- Female: $\hat{Math}=27.231-6.891*1+1.971*20$ = 59.76
- 66.651 - 59.76 = 6.891

Below, I've graphed the relationship between Self Esteem and Math Scores, controlling for gender.  Does it look like the regression lines from our model match the data? 

We can use the code below to plot the parallel regression lines (without the interaction).

```{r out.width="70%"}
data2$Gender <- as.factor(data2$Gender)

ggplot(data=data2,(aes(x=Self_Esteem,y=Math_Score,color=Gender)))+
  geom_point(alpha=.3)+
  geom_abline(intercept=27.230855,slope=1.971344,color="#f56269",lwd=.8)+
  geom_abline(intercept=(27.230855-6.891467),slope=1.971344,color="#0391a3",lwd=.8)+
  theme_minimal()
```

What can we do differently? Let's add an interaction to allow the slopes of *Self Esteem* to vary across *Gender*! To do this, all we have to do is add an asterisk. Our regression model looks REALLY different:

```{r}
mod2 <- lm(Math_Score~Self_Esteem*Gender,data=data2)
summary(mod2)
```

Here's a graph of the relationship (note that the default in ggplot is to plot the regression model with the interaction - aka, no assumption of parallel lines):

```{r out.width="70%"}
data2 %>% 
  ggplot(aes(x=Self_Esteem,y=Math_Score,color=Gender))+
  geom_point(alpha=.3)+
  geom_smooth(aes(color=Gender),method="lm",fill=NA)+
  theme_minimal()
```

**...Look how much better it fits the data!**

The interpretation of the regression coefficients is no longer straight forward.  We can no longer interpret the main effects (the regular regression coefficients) - we now have to interpret the **simple slopes**.  Let's plug and chug...

$$\hat{Math}=39.202-37.828*Gender_1+.966*Self-Esteem_i+2.041*Gender_1*Self-Esteem_i$$
**Male**

$\hat{y}=39.202-37.828*0+.966*Self-Esteem_i+2.041*0*Self-Esteem_i$

$\hat{y}=39.202+.966*Self-Esteem_i$

**Female**

$\hat{y}=39.202-37.828*1+.966*Self-Esteem_i+2.041*1*Self-Esteem_i$

$\hat{y}=1.374+.966*Self-Esteem_i+2.041*Self-Esteem_i$

$\hat{y}=1.374+3.007*Self-Esteem_i$

**Interpretation**

Gender and self-esteem were significant predictors of math score $[F(3,396)=171.476, p < .001, R^2=.565]$. There was a significant interaction between gender and self-esteem, which suggests that gender may serve as a moderator between self-esteem and math scores. Women with lower self esteem tend to have lower math scores than their male counterparts, while women with higher self esteem tend to have higher math scores than their male counterparts. As such, the slope of self-esteem for women ($\beta=3.007$) was much steeper than the slope of self-esteem for men ($\beta=.966$). 

**Hint**: You could also run an ANCOVA model (one continuous predictor and one categorical predictor with one continuous outcome). We haven't learned this yet.

## What if we have two categorical predictors?

We do the same thing as above, although it's much easier to plot the interaction because we only need to plug in 1's and 0's.

**Brief example**: Let's see if treatment for depression lowers scores on a depression assessment.  We have a treatment and control group, and pre-test and post-test data.  Let's assume there's an interaction between the two groups.

- Predictor 1: Time (Categorical: Pre test and post test)
- Predictor 2: Treat (Categorical: Treatment group and control group)
- Outcome: Depression (Continuous: Score on depression assessment)

**Run the regression model with an asterisk** 

```{r}
data2$Time <- factor(data2$Time,
                    levels=c(0,1),
                    labels=c("Pre","Post"))

data2$Treat <- factor(data2$Treat,
                     levels=c(0,1),
                     labels=c("Control","Treatment"))

mod3 <- lm(Depression~Time*Treat,data=data2)
summary(mod3)
```

**Let's get the marginal means.**

$$\hat{Depression}=\beta_0+\beta_1*X_{time}+\beta_2*X_{treat}+\beta_3*X_{time}*X_{treat}$$
$$\hat{Depression}=40.334-1.521*X_{time}-.925*X_{treat}-3.285*X_{time}*X_{treat}$$

- **Pre-test, Control**: $40.334-1.521*0-.925*0-3.285*0*0$ = 40.334

- **Post-test, Control**: $40.334-1.521*1-.925*0-3.285*1*0$ = 38.813

- **Pre-test, Treatment**: $40.334-1.521*0-.925*1-3.285*0*1$ = 39.409

- **Post-test, Treatment**: $40.334-1.521*1-.925*1-3.285*1*1$ = 34.603

**Double check using emmeans function**
```{r}
emmeans(mod3,~Time*Treat)
```

**Plot the relationship using emmip**

```{r out.width="60%"}
emmip(mod3,Treat~Time)
```

**Hint**: You could also run a two-way ANOVA, but we haven't learned this yet either.

**Interpretation**:

Since the interaction is statistically significant, we can conclude that the mean difference between the pre and post test scores is statistically significantly different for the treatment and control group.

Note that the mean difference in depression assessment scores for the treatment group is larger than the mean difference in depression assessment scores for the control group:

- **Mean difference for Control: Pre - Post = 40.334 - 38.813 = 1.521**
  - *The average reduction in depression symptoms for the control group is 1.521*.
- **Mean difference for Treatment: Pre - Post = 39.409 - 34.603 = 4.806**
  - *The average reduction in depression symptoms for the treatment group is 4.806*.
  
Because the interaction is significant, we know that the difference in these two mean differences (1.521 and 4.806) is statistically significant.  We therefore have evidence to suggest that treatment is more effective than no treatment in reducing depression over time.

## What if we have two continuous predictors?

We do the same as above, although the plots are a little more complicated.

**Brief Example**: Let's see if hours of sleep and SES are significant predictors of SAT scores. Let's assume there's an interaction between the two variables.

- Predictor 1: Hours_Sleep (Continuous)
- Predictor 2: Income_Bracket (Ordinal - Treat as continuous to avoid creating 8 dummy variables)
- Outcome: SAT_Score (Continuous)

**Run the regression model with an asterisk** 

```{r}
mod4 <- lm(SAT_Score~Income_Bracket*Hours_Sleep,data=data2)
summary(mod4)
```

- A positive and significant interaction means that the slope gets steeper as Income and Hours_Sleep increase. In other words, there's a multiplicative increase in SAT scores for people who are very wealthy and get a lot of sleep.

**Plot**

There are no categories to put on the x-axis or on the legend, so we have to be creative.  One way to plot this is to create three groups out of the moderator variable, where group one is -1 SD, group two is the mean, and group three is +1 SD.

```{r}
describe(data2$Income_Bracket)
```

Income Bracket Descriptives

- +1 SD: 4.97+1.32 = 6.29
- mean: 4.97
- -1 SD: 4.97-1.32 = 3.65

R will then plot the regression lines for each value of income:

```{r}
interact_plot(model=mod4,pred=Hours_Sleep,modx=Income_Bracket)
```
